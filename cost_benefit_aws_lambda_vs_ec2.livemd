<!-- livebook:{"persist_outputs":true} -->

# Cost-Benefit AWS Lambda vs EC2

```elixir
# Setup
Mix.install([
  {:vega_lite, "~> 0.1.6"},
  {:kino_vega_lite, "~> 0.1.11"}
])
```

## Overview

At some point, you may find yourself asking: When should I stop using a serverless compute service and start considering a virtual machine in the cloud? I am using AWS services as an example because they are the ones I am most familiar with.

This is our scenario (assumptions):

* We have a system that constantly executes jobs on a daily basis. A Job can be a combination of simple calculations, database query, http request, training your AI model, processing payments‚Ää-‚Ääyou name it.
* The system is intended to operate for less than a year.
* The jobs are critical and cannot be interrupted.
* We don't know the average duration of each job.

üèÅ The goal is to determine which service provides the best cost-benefit based on the computational load of our system.

To calculate the cost-benefit, we can start by asking: How many jobs can the cheapest EC2 instance handle compared to the cost of running the same number of jobs on a similarly configured Lambda?

If you're also curious about how to compare two services with different performance characteristics, keep reading ‚Äì we‚Äôll break it down step by step.

Let‚Äôs dive in.

## 1. Setup

### Average Job Duration

Based on our assumptions we don't know how long in average each job takes to complete. So, we must define our lower and upper bounds.

Arbitrarily, I will choose `1 ms/job` as lower bound. However, the upper bound must be `900,000 ms/job` (15 minutes) which is the maximum [Lambda timeout](https://docs.aws.amazon.com/lambda/latest/dg/configuration-timeout.html) meaning that a lambda cannot run jobs longer than 15 minutes.

```elixir
average_job_durations_ms = 1..900_000
```

### EC2 Setup

#### Rationale

Given our assumptions, the EC2 instance must run continuously, 24 hours a day, 7 days a week, for a period of less than one year. Additionally, since the jobs are critical and may vary in nature ‚Äì potentially being compute-intensive, memory-intensive, or storage-intensive ‚Äì we need an instance type that can handle a wide range of workloads efficiently.

To begin our cost comparison, it makes sense to choose a general-purpose instance, as these are versatile and suitable for a variety of tasks. Currently, the `t4g.nano` instance is the cheapest option that aligns with our requirements.

#### Settings

Region: `us-east-1`, Contract: `on-demand`, Instance Name: `t4g.nano`, Instance Type: `General Purpose`, Operating System: `linux`, Memory: `0.5gb`, vCPU: `2`.

[Price](https://aws.amazon.com/ec2/pricing/on-demand/): on-demand hourly: `0.0042 $/hour`.

#### Cost Calculation

The monthly cost can be easily calculated using AWS pricing fee with the following formula: `monthly_cost = price_per_hour * hours_per_day * days_per_month`.

```elixir
defmodule EC2 do
  def settings do
    %{
      memory_gb: 0.5,
      v_cpu: 2,
      hourly_price: 0.0042
    }
  end

  def get_monthly_cost() do
    ec2_settings = settings()
    hours_day = 24
    days_month = 30
    
    Float.round(ec2_settings.hourly_price * hours_day * days_month, 2)
  end
end

EC2.settings()
```

#### Lambda Setup

#### Rationale

Unlike EC2, which offers a variety of instance types to choose from, AWS Lambda only provides a single option for running functions. However, there is a key configuration parameter that significantly impacts performance: memory allocation.

According to the [Lambda Memory Configuration](https://docs.aws.amazon.com/lambda/latest/dg/configuration-memory.html) documentation, the number of CPUs assigned to a Lambda function is directly related to the amount of memory configured. Unfortunately, AWS does not provide a clear table specifying how many vCPUs correspond to a given memory allocation.

However, insights from a study focused on [optimizing Lambda cost](https://web.archive.org/web/20220629183438/https://www.sentiatechblog.com/aws-re-invent-2020-day-3-optimizing-lambda-cost-with-multi-threading?utm_source=reddit&utm_medium=social&utm_campaign=day3_lambda) indicate that when the memory is set between 128 MB and 3,008 MB, Lambda functions utilize approximately 2 vCPUs. This is significant because it matches the vCPU count of our chosen EC2 instance, and the memory range (512 MB) also aligns with the configuration of our EC2 instance.

Therefore, based on this analysis, the following setup is recommended:

#### Setup

Region: `us-east-1`, Architecture: `x86`, Memory: `512 mb`, Ephemeral Storage: `512 mb` (default).

[Price](https://aws.amazon.com/lambda/pricing/): request price: `0.0000002 request`, execution price: `0.0000166667 gb/second`.

Where the request price is the number of jobs you request a lambda to execute and the execution price is the time it takes to complete the job.

#### Cost Calculation

Assuming that cold starts do not impact performance and that jobs nearing the timeout limit complete without unexpected interruptions, the function `calculate_monthly_costs()` estimates the total monthly cost of running a Lambda function. This calculation is based on two key inputs:

* The total number of job requests per month.
* The average execution time per job.

The function follows the same pricing model as outlined in the [Lambda Cost Calculator](https://calculator.aws/#/createCalculator/Lambda), taking into account compute time, request count, and ephemeral storage if applicable.

```elixir
defmodule Lambda do
  @type average_job_duration_ms :: integer()
  @type requests :: integer()
  @type cost :: float()
  @type cost_scenario :: {average_job_duration_ms, requests, cost}

  def settings do
    %{
      memory_mb: 512,
      ephemeral_storage_mb: 512,
      execution_price_gb_sec: 0.0000166667,
      request_price: 0.0000002,
      ephemeral_storage_price_gb: 0.0000000309,
    }
  end

  @spec calculate_monthly_costs(requests_month :: integer(), avg_request_duration_ms :: integer()) :: cost_scenario()
  def calculate_monthly_costs(requests_month, avg_request_duration_ms) do
    # Unit conversions
    mb_to_gb = 0.0009765625
    ms_to_s = 0.001

    # Free tiering
    free_compute_tier_gb_s = 400000
    free_request_tier = 1000000
    free_ephemeral_storage_tier_gb = 0.5

    # Lambda settings
    lambda_settings = settings()

    # Compute price calculations
    memory_allocated = lambda_settings.memory_mb * mb_to_gb
    total_compute_s = requests_month * avg_request_duration_ms * ms_to_s
    billable_compute = max(memory_allocated * total_compute_s - free_compute_tier_gb_s, 0)
    compute_cost = billable_compute * lambda_settings.execution_price_gb_sec

    # Request price calculations
    billable_requests = max(requests_month - free_request_tier, 0)
    request_cost = billable_requests * lambda_settings.request_price

    # Ephemeral storage calculations
    ephemeral_storage_allocated = lambda_settings.ephemeral_storage_mb * mb_to_gb
    billable_ephemeral_storage = max(ephemeral_storage_allocated - free_ephemeral_storage_tier_gb, 0)
    ephemeral_storage_cost = billable_ephemeral_storage * total_compute_s * lambda_settings.ephemeral_storage_price_gb

    # Total cost
    total_cost = compute_cost + request_cost + ephemeral_storage_cost
    Float.round(total_cost, 2)
  end
end

Lambda.settings()
```

## 2. Cost Calculations

To calculate EC2 and Lambda costs we will use different scenarios.

A scenario represents a single possible configuration of the maximum capacity for a workload that the EC2 instance can handle and its associated cost. Specifically, it defines:

The cost of executing a given number of jobs, where each job takes a certain average time to complete. Each scenario is encapsulated as a map in the form:

`%{
  average_job_duration_ms: <ajd>,
  jobs_month: <jm>,
  cost: <c>
}`.

<!-- livebook:{"break_markdown":true} -->

### Scenarios

```elixir
alias VegaLite, as: Vl

# Milliseconds in a month (24 hours/day * 30 days/month).
ms_month = :timer.hours(24 * 30)
ec2_settings = EC2.settings()

scenarios = Enum.map(average_job_durations_ms, fn ajd_ms ->
  jobs_month = round(ms_month / ajd_ms * ec2_settings.v_cpu)

  %{
    average_job_duration_ms: ajd_ms,
    jobs_month: jobs_month,
    cost: nil
  }
end)
```

```elixir
Vl.new(width: 400, height: 400, title: "Cost Scenarios")
|> Vl.data_from_values(scenarios)
|> Vl.concat(
  [
    # Chart 1 unclipped
    Vl.new(title: "(Unclipped)")
    |> Vl.mark(:line, tooltip: [content: "data"])
    |> Vl.encode_field(
      :x, "average_job_duration_ms",
      scale: [type: "pow"],
      axis: [title: "Average Job Durations (ms)"]
    )
    |> Vl.encode_field(
      :y, "jobs_month",
      scale: [type: "pow"],
      title: "Jobs per Month"
    ),
    # Chart 2 clipped
    Vl.new(title: "(Clipped)")
    |> Vl.mark(:line, clip: true, tooltip: [content: "data"])
    |> Vl.encode_field(
      :x, "average_job_duration_ms",
      scale: [type: "pow", domain: [0, 400000]],
      axis: [title: "Average Job Durations (ms)"]
    )
    |> Vl.encode_field(
      :y, "jobs_month",
      scale: [type: "pow", domain: [0, 1000000]],
      title: "Jobs per Month"
    )
  ],
  :horizontal
)
```

From the charts we can tell the following:

* `Inverse Relationship` Between Job Duration and Job Throughput: longer jobs translate into fewer total jobs.
* The `max` amount of jobs is 5,184,000,000 jobs/month when each job takes 1 ms to complete.
* The `min` amount of jobs is 5,760 jobs/month when each job takes 900,000 ms to complete.
* The curve shows a `decreasing rate of change` between ~400,000 and ~200,000 jobs/month, meaning that while job throughput continues to decline as duration increases, the decline becomes less steep. This suggests diminishing returns and indicates that the system is approaching a lower bound in throughput.
* In the range between ~5,000,000,000 and ~400,000 jobs/month, the curve exhibits a `steep negative slope, indicating a high sensitivity of job throughput to changes in duration`. In this region, small increases in average job duration lead to substantial decreases in throughput.
* When the average job duration exceeds ~200,000ms, the `curve begins to flatten, indicating a reduced sensitivity of throughput to duration`. In this region, further increases in job duration result in only marginal decreases in job throughput.

<!-- livebook:{"break_markdown":true} -->

### EC2 Cost

```elixir
ec2_cost_scenarios = Enum.map(scenarios, fn scenario ->
    Map.put(scenario, :cost, EC2.get_monthly_cost())
  end)
```

```elixir
Vl.new(width: 400, height: 400, title: "Cost Scenarios (Clipped)")
|> Vl.data_from_values(ec2_cost_scenarios)
|> Vl.encode_field(:x, "average_job_duration_ms",
    scale: [type: "pow", domain: [0, 400000]],
    axis: [title: "Average Job Durations (ms)"]
)
|> Vl.layers([
  Vl.new()
  |> Vl.mark(:line, color: "blue", clip: true, tooltip: [content: "data"])
  |> Vl.encode_field(:y, "jobs_month",
    scale: [type: "pow", domain: [0, 1000000]],
    title: "Jobs per Month",
    axis: [title_color: "blue"]
  ),
  Vl.new()
  |> Vl.mark(:line, color: "green", clip: true, tooltip: [content: "data"])
  |> Vl.encode_field(:y, "cost",
    title: "Monthly Cost (USD)",
    axis: [title_color: "green"]
  )
])
|> Vl.resolve(:scale, y: :independent)
```

Looking at the chart we can tell the following:

* The EC2 instance can support `5,184,000,000 jobs/month` when each job takes `1 ms` and `5,760 jobs/month` when each job takes `900,000 ms` (15 minutes) to complete. All for the same fixed cost of `3.02 $/month`. In other words, billing is based on provisioned time, not actual usage or workload intensity.

<!-- livebook:{"break_markdown":true} -->

### Lambda Cost

```elixir
lambda_cost_scenarios = Enum.map(scenarios, fn %{average_job_duration_ms: ajd, jobs_month: jm} = scenario ->
    Map.put(scenario, :cost, Lambda.calculate_monthly_costs(jm, ajd))
  end)
```

```elixir
Vl.new(width: 400, height: 400)
|> Vl.data_from_values(lambda_cost_scenarios)
|> Vl.concat(
  [
    # Chart 1 scenarios
    Vl.new(title: "Cost Scenarios")
    |> Vl.mark(:line, color: "blue", clip: true, tooltip: [content: "data"])
    |> Vl.encode_field(
      :x, "average_job_duration_ms",
      scale: [type: "pow"],
      axis: [title: "Average Job Durations (ms)"]
    )
    |> Vl.encode_field(
      :y, "jobs_month",
      scale: [type: "pow"],
      title: "Jobs per Month",
      axis: [title_color: "blue"]
    ),
    # Chart 2 costs
    Vl.new(title: "Cost per Scenario (log scale)")
    |> Vl.mark(:line, color: "green", clip: true, tooltip: [content: "data"])
    |> Vl.encode_field(
      :x, "average_job_duration_ms",
      scale: [type: "log"],
      axis: [title: "Cost Scenario"]
    )
    |> Vl.encode_field(
      :y, "cost",
      scale: [type: "log"],
      title: "Monthly Cost (USD)",
      axis: [title_color: "green"]
    )
  ],
  :horizontal
)
```

```elixir
# Expensivest and cheapest scenarios
{cheapest_scenario, expensivest_scenario} = lambda_cost_scenarios |> Enum.min_max_by(& &1.cost)
IO.inspect(expensivest_scenario, label: "Expensivest scenario: ")
IO.inspect(cheapest_scenario, label: "Cheapest scenario: ")
```

```elixir
# Possible cheapest scenarios
lambda_cost_scenarios
|> Enum.count(fn %{cost: c} -> c == cheapest_scenario.cost end)
|> IO.inspect(label: "Number of possible cheapest scenarios: ")
```

Looking at the charts we can tell the following:

* When each job has an average duration of `1 ms`, AWS Lambda can handle up to `5,184,000,000 jobs/month`, resulting in a monthly cost of approximately `$1,073.13`.
* On the opposite end of the spectrum, with jobs averaging `900,000 ms` (or 15 minutes), the system can only support around `5,760 jobs/month`, at a much lower cost of `$36.53`.
* This illustrates that Lambda‚Äôs cost is highly sensitive to the volume of short-duration jobs. As job duration decreases, the number of jobs that can be processed increases dramatically, and cost scales up accordingly.
* Interestingly, the minimum cost point (i.e., the most cost-efficient configuration) occurs in `799,695` different scenarios, highlighting that many long-running job configurations converge to the same low-cost region, making cost relatively flat in that range.

<!-- livebook:{"break_markdown":true} -->

### EC2 vs Lambda Costs

```elixir
comparison_cost_scenarios = Enum.flat_map(lambda_cost_scenarios, fn scenario ->
    updated_scenario = Map.put(scenario, :service, "Lambda")
    
    new_scenario = 
      scenario
      |> Map.put(:cost, EC2.get_monthly_cost())
      |> Map.put(:service, "EC2")

    [updated_scenario, new_scenario]
  end)
```

```elixir
Vl.new(width: 400, height: 400, title: "EC2 vs Lambda Cost")
|> Vl.data_from_values(comparison_cost_scenarios)
|> Vl.concat(
  [
    # Chart 1
    Vl.new(title: "(Unclipped)")
    |> Vl.mark(:line, clip: true, tooltip: [content: "data"])
    |> Vl.encode_field(
      :x, "average_job_duration_ms",
      type: :quantitative,
      axis: [title: "Cost Scenario"]
    )
    |> Vl.encode_field(
      :y, "cost",
      type: :quantitative,
      title: "Service Cost"
    )
    |> Vl.encode_field(:color, "service", type: :nominal),
    # Chart 2
    Vl.new(title: "(Clipped)")
    |> Vl.mark(:line, clip: true, tooltip: [content: "data"])
    |> Vl.encode_field(
      :x, "average_job_duration_ms",
      type: :quantitative,
      scale: [type: "pow", domain: [0, 1000]],
      axis: [title: "Cost Scenario"]
    )
    |> Vl.encode_field(
      :y, "cost",
      scale: [domain: [0, 100]],
      type: :quantitative,
      title: "Service Cost"
    )
    |> Vl.encode_field(:color, "service", type: :nominal)
  ],
  :horizontal
)
```

Looking at the charts we can tell the following:

* The `anomalous spikes observed between` ~400,000 and ~450,000 jobs/month are attributable to minor floating-point variations in the Lambda cost calculations (e.g., `$36.53` vs. `$36.54`), likely due to rounding artifacts.
* The Lambda and EC2 cost `curves do not intersect at any point`, indicating that there is no workload range where both services offer identical cost efficiency.
* The `minimum observed cost delta` between Lambda and EC2 is `$33.51`, calculated as cheapest lambda scenario minus EC2 cost.

## 3. Final Thoughts

Based on the scenario for the analysis, the choice between AWS Lambda and EC2 primarily hinges on workload characteristics and system capacity.

* `Service Choice Rule of Thumb`: Under a high constant workload, the price difference between Lambda and a t2.nano EC2 instance can be significant, with Lambda potentially being more expensive for continuous, long-running tasks. However, if the workload is bursty or short-lived, Lambda could be more cost-effective.

* `Computational Capacity Bound`: Since we are simulating a similarly compute capacity for both services, the maximum job throughput is inherently limited by job duration. In practice, this can easily vary depending on the CPU performance, memory, and memory bandwith.

* `Cost Efficiency Consideration`: Savings can be substantial, not just in absolute terms (like $33.51), but as a percentage of total costs. In our case around ~1200% ($3.02/$36.56) costs can be saved using EC2 against Lambda.

* `Lambda Pricing Threshold`: The execution price of Lambda increases after the first 6 billion GB-seconds. However, in our current context, this is not a concern, as the EC2 instance capacity (~5.1 billion GB-seconds) is the limiting factor.

* `Scalability Consideration`: If the computational demand exceeds the capacity of the cheapest EC2 instance, it is more practical to scale vertically by selecting a higher-tier EC2 instance or horizontally with auto-scaling groups rather than continuing to use Lambda when your system is under a constinuos workload.
